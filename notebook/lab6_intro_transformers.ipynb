{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03662255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\subject\\nlp\\code\\natural_language_processing\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb73d49",
   "metadata": {},
   "source": [
    "# Masked token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1bf034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\subject\\nlp\\code\\natural_language_processing\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu gốc: Hanoi is the <mask> of Vietnam.\n",
      "Dự đoán: 'capital' với độ tin cậy: 0.9354\n",
      " -> Câu hoàn chỉnh: Hanoi is the capital of Vietnam.\n",
      "Dự đoán: 'center' với độ tin cậy: 0.0251\n",
      " -> Câu hoàn chỉnh: Hanoi is the center of Vietnam.\n",
      "Dự đoán: 'heart' với độ tin cậy: 0.0109\n",
      " -> Câu hoàn chỉnh: Hanoi is the heart of Vietnam.\n",
      "Dự đoán: 'centre' với độ tin cậy: 0.0032\n",
      " -> Câu hoàn chỉnh: Hanoi is the centre of Vietnam.\n",
      "Dự đoán: 'city' với độ tin cậy: 0.0030\n",
      " -> Câu hoàn chỉnh: Hanoi is the city of Vietnam.\n"
     ]
    }
   ],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", model=\"xlm-roberta-base\")\n",
    "input_sentence = \"Hanoi is the <mask> of Vietnam.\"\n",
    "\n",
    "preds = mask_filler(input_sentence, top_k=5)\n",
    "\n",
    "print(f\"Câu gốc: {input_sentence}\")\n",
    "for pred in preds:\n",
    "    print(f\"Dự đoán: '{pred['token_str']}' với độ tin cậy: {pred['score']:.4f}\")\n",
    "    print(f\" -> Câu hoàn chỉnh: {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04ef2e",
   "metadata": {},
   "source": [
    "# Next token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b488cdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu mồi: 'The best thing about learning NLP is'\n",
      "Văn bản được sinh ra:\n",
      "The best thing about learning NLP is that it offers a way to understand and interpret how people communicate, including nonverbal cues, emotions, and language. It's not just about the language itself, but how people use it. The field is rapidly growing, and it's becoming more and more important in our daily lives. It's a powerful tool for professionals who work with language, including those in education, business, and healthcare.\n",
      "\n",
      "NLP can be used in multiple ways, including in the workplace, in education, and in healthcare\n"
     ]
    }
   ],
   "source": [
    "# 1. Tải pipeline \"text-generation\"\n",
    "# Pipeline này sẽ tự động tải một mô hình phù hợp (thường là GPT-2)\n",
    "generator = pipeline(\"text-generation\", model=\"Qwen/Qwen3-0.6B\")\n",
    "\n",
    "# 2. Đoạn văn bản mồi\n",
    "prompt = \"The best thing about learning NLP is\"\n",
    "\n",
    "# 3. Sinh văn bản\n",
    "# max_length: tổng độ dài của câu mồi và phần được sinh ra\n",
    "# num_return_sequences: số lượng chuỗi kết quả muốn nhận\n",
    "generated_texts = generator(prompt, max_new_tokens=100, num_return_sequences=1)\n",
    "\n",
    "# 4. In kết quả\n",
    "print(f\"Câu mồi: '{prompt}'\")\n",
    "for text in generated_texts:\n",
    "    print(\"Văn bản được sinh ra:\")\n",
    "    print(text['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5fbf3d",
   "metadata": {},
   "source": [
    "# Sentence Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54edf3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2023,  2003,  1037,  7099,  6251,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  3086,  2015, 15871,  2044,  1996,  3086,  3730, 17848,  1010,\n",
      "          2109,  2000, 24134,  1996, 18215,  2779,  1999,  1996,  2969,  1011,\n",
      "          3086,  4641,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Last hiddent state: torch.Size([2, 24, 768])\n",
      "Attention mask shape: torch.Size([2, 24])\n",
      "Vector biểu diễn của câu:\n",
      "tensor([[-0.0639, -0.4284, -0.0668,  ..., -0.1753, -0.1239,  0.3197],\n",
      "        [-0.1905, -0.3300,  0.3282,  ...,  0.0159, -0.0221, -0.1398]])\n",
      "\n",
      "Kích thước của vector: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# 1. Chọn một mô hình BERT\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 2. Câu đầu vào\n",
    "sentences = [\n",
    "    \"This is a sample sentence.\",\n",
    "    \"Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\"\n",
    "             \n",
    "]\n",
    "\n",
    "# 3. Tokenize câu\n",
    "# padding=True: đệm các câu ngắn hơn để có cùng độ dài\n",
    "# truncation=True: cắt các câu dài hơn\n",
    "# return_tensors='pt': trả về kết quả dưới dạng PyTorch tensors\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "print(inputs)\n",
    "\n",
    "# 4. Đưa qua mô hình để lấy hidden states\n",
    "# torch.no_grad() để không tính toán gradient, tiết kiệm bộ nhớ\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# outputs.last_hidden_state chứa vector đầu ra của tất cả các token\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "# shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "# 5. Thực hiện Mean Pooling\n",
    "# Để tính trung bình chính xác, chúng ta cần bỏ qua các token đệm (padding tokens)\n",
    "attention_mask = inputs['attention_mask']\n",
    "print(\"Last hiddent state:\", last_hidden_state.shape)\n",
    "print(\"Attention mask shape:\", attention_mask.shape)\n",
    "\n",
    "mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n",
    "sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "sentence_embedding = sum_embeddings / sum_mask\n",
    "\n",
    "# 6. In kết quả\n",
    "print(\"Vector biểu diễn của câu:\")\n",
    "print(sentence_embedding)\n",
    "print(\"\\nKích thước của vector:\", sentence_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18383fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e466b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda3b2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6100416b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
